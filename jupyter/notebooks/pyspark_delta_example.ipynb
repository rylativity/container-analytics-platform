{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Usage with Delta Lake & Minio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to write a CSV file directly to Minio, and also how to write and read a managed Delta Lake table in Minio.\n",
    "\n",
    "Click the Table of Contents button in the left JupyterLab sidebar (the button on the far left of this browser window that looks like a bulleted list) to see the types of examples provided. **Make sure to run all the cells above a given section, since most examples in this notebook depend on those above them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Environment Variables for Minio (S3) Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'HOSTNAME': 'a7e8edb448e6',\n",
       "        'AWS_ACCESS_KEY_ID': 'jupyteraccesskey',\n",
       "        'AWS_SECRET_ACCESS_KEY': 'jupytersupersecretkey',\n",
       "        'AWS_ENDPOINT_URL': 'http://minio:9000',\n",
       "        'S3_BUCKET': 'test',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D',\n",
       "        'PYTHON_VERSION': '3.11.6',\n",
       "        'PYTHON_PIP_VERSION': '23.2.1',\n",
       "        'PYTHON_SETUPTOOLS_VERSION': '65.5.1',\n",
       "        'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/9af82b715db434abb94a0a6f3569f43e72157346/public/get-pip.py',\n",
       "        'PYTHON_GET_PIP_SHA256': '45a2bb8bf2bb5eff16fdd00faef6f29731831c7c59bd9fc2bf1f3bed511ff1fe',\n",
       "        'HOME': '/root',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_SESSION_NAME': '/notebooks/pyspark_delta_example.ipynb',\n",
       "        'JPY_PARENT_PID': '1',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ \n",
    "## Should see AWS_ENDPOINT_URL, AWS_ACCESS_KEY_ID, and AWS_SECRET_ACCESS_KEY environment varibles.\n",
    "# These environment variables are set in the docker-compose.yml, and the service account used by PySpark\n",
    "#> to read from and write to Minio are created by the minio-init container defined in docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\")\n",
    "AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_ENDPOINT_URL = os.environ.get(\"AWS_ENDPOINT_URL\")\n",
    "# AWS_ACCESS_KEY_ID = \"sparkaccesskey\"\n",
    "# S3_BUCKET = \"test\"\n",
    "# AWS_SECRET_ACCESS_KEY = \"sparksupersecretkey\"\n",
    "# AWS_ENDPOINT_URL = \"http://minio:9000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Pyspark to Connect to Minio and Enable Delta-Lake Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-df536035-8985-4a6f-a615-52193b00baaf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.3/hadoop-aws-3.3.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.3!hadoop-aws.jar (74ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.1.0/delta-core_2.12-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.1.0!delta-core_2.12.jar (159ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1026/aws-java-sdk-bundle-1.11.1026.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.1026!aws-java-sdk-bundle.jar (7795ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (47ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.1.0/delta-storage-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.1.0!delta-storage.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (24ms)\n",
      ":: resolution report :: resolve 2008ms :: artifacts dl 8164ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.3 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   7   |   7   |   0   ||   7   |   7   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-df536035-8985-4a6f-a615-52193b00baaf\n",
      "\tconfs: [default]\n",
      "\t7 artifacts copied, 0 already retrieved (226188kB/118ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/14 14:45:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# This cell may take some time to run the first time, as it must download the necessary spark jars\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "## IF YOU ARE USING THE SPARK CONTAINERS, UNCOMMENT THE LINE BELOW TO OFFLOAD EXECUTION OF SPARK TASKS TO SPARK CONTAINERS\n",
    "#conf.setMaster(\"spark://spark:7077\")\n",
    "\n",
    "conf.set(\"spark.jars.packages\", 'org.apache.hadoop:hadoop-aws:3.3.3,io.delta:delta-core_2.12:2.1.0')\n",
    "# conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "conf.set('spark.hadoop.fs.s3a.endpoint', AWS_ENDPOINT_URL)\n",
    "conf.set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "conf.set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)\n",
    "conf.set('spark.hadoop.fs.s3a.path.style.access', \"true\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Sample CSV Data from Local Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/data/appl_stock.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in [\"Open\", \"High\", \"Low\", \"Close\", \"AdjClose\"]:\n",
    "    df = df.withColumn(col,df[col].cast('double'))\n",
    "for col in [\"Volume\"]:\n",
    "    df = df.withColumn(col, df[col].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write CSV Directly to Minio (Not as a Delta Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.csv(f\"s3a://{S3_BUCKET}/appl_stock.csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigate to http://localhost:9090 and login to the Minio Console to see the CSV file**\n",
    "\n",
    "(username and password for minio can be found in the environment variables section of the minio service definition in the docker-compose.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a Delta Lake Table in Minio using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Have to replace spaces in column names with underscores for Delta\n",
    "delta_df = df\n",
    "for col in delta_df.columns:\n",
    "    delta_df = delta_df.withColumnRenamed(col, col.replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Month and Year columns for partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_df = delta_df.withColumn(\"Month\", month(delta_df.Date))\n",
    "delta_df = delta_df.withColumn(\"Year\", year(delta_df.Date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table_name = \"appl_stock_delta_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_df.write.format(\"delta\").partitionBy('Year','Month').option(\"overwriteSchema\", \"true\").save(f\"s3a://{S3_BUCKET}/{delta_table_name}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigate to http://localhost:9090 and login to the Minio Console to see the Delta Lake Table**\n",
    "\n",
    "**Note that the Delta Lake Table includes both the data partitions and the metadata log**\n",
    "\n",
    "(username and password for minio can be found in the environment variables section of the minio service definition in the docker-compose.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Delta Table Back into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_delta_df = spark.read.format(\"delta\").load(f\"s3a://{S3_BUCKET}/{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_delta_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Data From Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, f\"s3a://{S3_BUCKET}/{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table.delete(\"Date < '2010-02-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delta_table.vacuum()\n",
    "\n",
    "# .vacuum() is not really necessary for this example. For more info, see https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_df = delta_table.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_df.describe().show()\n",
    "# Notice the min date due to the delete above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Time Travel to READ a Previous Version of the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "previous_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(f\"s3a://{S3_BUCKET}/{delta_table_name}\")\n",
    "previous_df.describe().show()\n",
    "# Notice the min date, showing that we are reading from a previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Time Travel to RESTORE a Previous Version of the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Capture a timestamp before we restore the delta table so we can see how to use a timestamp to do restore later on\n",
    "pre_restore_time = datetime.now().strftime(\"%Y-%m-%d %X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restore to a numbered version, and show the result summary of the restore operation\n",
    "delta_table.restoreToVersion(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## We can always un-restore (or restore a more recent version) because restoring is a metadata-only operation\n",
    "##  (i.e. the data files themselves are not modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#delta_table.restoreToVersion(1)\n",
    "\n",
    "# Instead of using restoreToVersion, we can use a timestamp to revert to the table as it was at a specific time\n",
    "delta_table.restoreToTimestamp(pre_restore_time).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Trino to Automatically Infer Schema from Delta Table and Make Data Available for End User Querying / Dashboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table_name = \"appl_stock_delta_table\"\n",
    "delta_schema_name = \"my_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility function to simplify query execution against Trino REST API\n",
    "def execute_trino_query(query, statement_endpoint = \"http://trino:8080/v1/statement\", user = \"admin\", password = \"\"):\n",
    "    \n",
    "    print(f\"Executing query:\\n{query}\")\n",
    "    res = requests.post(statement_endpoint,data = query.encode(\"UTF8\"), auth=requests.auth.HTTPBasicAuth(user,password))\n",
    "    \n",
    "    data = []\n",
    "    cols = None\n",
    "    while True:\n",
    "        json_res = res.json()\n",
    "        state = json_res.get(\"stats\").get(\"state\")\n",
    "        print(f\"State: {state}\")\n",
    "\n",
    "        res_data = json_res.get(\"data\")\n",
    "        if res_data:\n",
    "            data.extend(res_data)\n",
    "        \n",
    "        res_cols = json_res.get(\"columns\")\n",
    "        if res_cols:\n",
    "            cols = [i[\"name\"] for i in res_cols]\n",
    "            \n",
    "        next_uri = json_res.get(\"nextUri\")\n",
    "        if next_uri:\n",
    "            sleep(.5)\n",
    "            res = requests.get(next_uri)\n",
    "        else:\n",
    "            if state == \"FAILED\":\n",
    "                raise Exception(res.content)\n",
    "            return [dict(zip(cols, d)) for d in data]\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trigger Trino to Read Delta Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_schema_statement = f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS delta.my_schema\n",
    "WITH (location = 's3a://{S3_BUCKET}/')\n",
    "\"\"\"\n",
    "\n",
    "register_table_statement = f\"\"\"CALL delta.system.register_table(schema_name => '{delta_schema_name}', table_name => '{delta_table_name}', table_location => 's3a://{S3_BUCKET}/{delta_table_name}')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for query in [create_schema_statement, register_table_statement]:\n",
    "    print(execute_trino_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data from Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIMIT = 10\n",
    "select_statement = f\"SELECT * FROM delta.{delta_schema_name}.{delta_table_name}\"\n",
    "if LIMIT and type(LIMIT) == int:\n",
    "    select_statement += f\" LIMIT {LIMIT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = execute_trino_query(select_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a New Delta Lake Table Using Trino 'CREATE TABLE AS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statement = f\"CREATE TABLE delta.{delta_schema_name}.{delta_table_name}_copy AS (SELECT * FROM delta.{delta_schema_name}.{delta_table_name} LIMIT 10)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = execute_trino_query(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statement = f\"SELECT * FROM delta.{delta_schema_name}.{delta_table_name}_copy LIMIT 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = execute_trino_query(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Superset API To Add Connection to Trino Delta Lake Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: THE STEPS BELOW WILL ONLY WORK IF YOU ARE ALSO USING THE SUPERSET CONTAINERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERSET_BASE_URL = \"http://superset:8088\"\n",
    "TOKEN_ENDPOINT = f\"{SUPERSET_BASE_URL}/api/v1/security/login\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"password\": \"admin\",\n",
    "  \"provider\": \"db\",\n",
    "  \"refresh\": True,\n",
    "  \"username\": \"admin\"\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\":\"application/json\",\n",
    "    \"Accept\":\"application/json\"\n",
    "}\n",
    "res = requests.post(TOKEN_ENDPOINT, data=json.dumps(data), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = res.json()[\"access_token\"]\n",
    "headers[\"Authorization\"] = f\"Bearer {auth_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Disabled CSRF Token in Superset config.py\n",
    "\n",
    "# CSRF_ENDPOINT = f\"{SUPERSET_BASE_URL}/api/v1/security/csrf_token/\"\n",
    "\n",
    "# res = requests.get(CSRF_ENDPOINT,headers=headers)\n",
    "\n",
    "# csrf_token = res.json()[\"result\"]\n",
    "# headers[\"X-CSRFToken\"] = csrf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_ENDPOINT = f\"{SUPERSET_BASE_URL}/api/v1/database/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"database_name\": \"Delta\",\n",
    "    \"engine\": \"trino\",\n",
    "    \"configuration_method\": \"sqlalchemy_form\",\n",
    "    \"catalog\": [\n",
    "        {\n",
    "            \"name\": \"\",\n",
    "            \"value\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"sqlalchemy_uri\": \"trino://trino@trino:8080/delta\",\n",
    "    \"expose_in_sqllab\": True,\n",
    "    \"allow_ctas\": True,\n",
    "    \"allow_cvas\": True,\n",
    "    \"allow_dml\": True,\n",
    "    \"extra_json\": {\n",
    "        \"allows_virtual_table_explore\": True\n",
    "    },\n",
    "    \"extra\": '{\"allows_virtual_table_explore\":true,\"metadata_params\":{},\"engine_params\":{},\"schemas_allowed_for_file_upload\":[]}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(DATABASE_ENDPOINT, data=json.dumps(data), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d04c0d231515a3f3089cb8e547ddeb6d0ef1c95d45851d9802a6754ba01b59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
